{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "# Activation Function\n",
    "\n",
    "Sigmoid :\n",
    "sigma(z) = 1/(1+ e**(-z))\n",
    "\n",
    "\n",
    "\n",
    "# Loss function\n",
    "L(y_pred, y) = 1/2 * (y-y_pred)**2\n",
    "# Square Loss Function\n",
    "Very basic\n",
    "But does not work well with gradent decent as it is not a convex function and so has multiple\n",
    "minimun\n",
    "\n",
    "So we use log likelyhood\n",
    "\n",
    "# Log Loss Function\n",
    "\n",
    "L(y_pred, y) = - (y*log(y_pred) + (1-y)*  log(1-y_pred))\n",
    "\n",
    "Since we want L to be small:\n",
    "If y = 1 => 1-y = 0 => L = -log(y_pred)  so we want large value of y_pred\n",
    "\n",
    "if y = 0 => y*log(y_pred) = 0 => L = - log(1-y_pred) so we want y_pred to be small\n",
    "\n",
    "\n",
    "# Cost function\n",
    "\n",
    "J(w, b) = 1/m * Sum(i=1 to m) L(y_pred[i], y[i])\n",
    "\n",
    "\n",
    "# Gradient Descent\n",
    "We want to minimize the cost function J(w,b)\n",
    "\n",
    "Repeat:\n",
    "{\n",
    "    w = w - alpha * d(J(w))/dw\n",
    "\n",
    "}\n",
    "\n",
    "keep decreacing or increasing w by slope * learning rate alpha\n",
    "if slope is positive, means w is increating, so decrease we need it to substract and vice versa"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
